{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KIMBERA MF Equity</th>\n",
       "      <th>GAPB MF Equity</th>\n",
       "      <th>CEMEXCPO MF Equity</th>\n",
       "      <th>ASURB MF Equity</th>\n",
       "      <th>GFNORTEO MF Equity</th>\n",
       "      <th>KOFUBL MF Equity</th>\n",
       "      <th>AC* MF Equity</th>\n",
       "      <th>LABB MF Equity</th>\n",
       "      <th>FEMSAUBD MF Equity</th>\n",
       "      <th>OMAB MF Equity</th>\n",
       "      <th>...</th>\n",
       "      <th>MEGACPO MF Equity</th>\n",
       "      <th>GCC* MF Equity</th>\n",
       "      <th>BIMBOA MF Equity</th>\n",
       "      <th>GMEXICOB MF Equity</th>\n",
       "      <th>GCARSOA1 MF Equity</th>\n",
       "      <th>TLEVICPO MF Equity</th>\n",
       "      <th>ALFAA MF Equity</th>\n",
       "      <th>BSMXB MF Equity</th>\n",
       "      <th>PINFRA* MF Equity</th>\n",
       "      <th>MEXBOL INDEX</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>40.37</td>\n",
       "      <td>152.05</td>\n",
       "      <td>8.72</td>\n",
       "      <td>243.89</td>\n",
       "      <td>94.99</td>\n",
       "      <td>123.90</td>\n",
       "      <td>104.68</td>\n",
       "      <td>13.85</td>\n",
       "      <td>161.63</td>\n",
       "      <td>83.61</td>\n",
       "      <td>...</td>\n",
       "      <td>64.19</td>\n",
       "      <td>44.81</td>\n",
       "      <td>45.95</td>\n",
       "      <td>36.79</td>\n",
       "      <td>70.98</td>\n",
       "      <td>94.34</td>\n",
       "      <td>34.10</td>\n",
       "      <td>30.25</td>\n",
       "      <td>202.73</td>\n",
       "      <td>42977.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>39.16</td>\n",
       "      <td>148.57</td>\n",
       "      <td>8.43</td>\n",
       "      <td>238.29</td>\n",
       "      <td>91.79</td>\n",
       "      <td>120.29</td>\n",
       "      <td>102.09</td>\n",
       "      <td>14.41</td>\n",
       "      <td>157.62</td>\n",
       "      <td>81.77</td>\n",
       "      <td>...</td>\n",
       "      <td>64.45</td>\n",
       "      <td>43.51</td>\n",
       "      <td>45.02</td>\n",
       "      <td>35.71</td>\n",
       "      <td>69.83</td>\n",
       "      <td>92.62</td>\n",
       "      <td>33.74</td>\n",
       "      <td>29.46</td>\n",
       "      <td>202.93</td>\n",
       "      <td>42113.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>38.89</td>\n",
       "      <td>147.93</td>\n",
       "      <td>8.31</td>\n",
       "      <td>233.03</td>\n",
       "      <td>91.72</td>\n",
       "      <td>118.82</td>\n",
       "      <td>101.12</td>\n",
       "      <td>13.77</td>\n",
       "      <td>157.80</td>\n",
       "      <td>81.58</td>\n",
       "      <td>...</td>\n",
       "      <td>63.16</td>\n",
       "      <td>43.51</td>\n",
       "      <td>45.77</td>\n",
       "      <td>36.45</td>\n",
       "      <td>70.70</td>\n",
       "      <td>91.89</td>\n",
       "      <td>33.99</td>\n",
       "      <td>29.72</td>\n",
       "      <td>203.56</td>\n",
       "      <td>42041.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>39.13</td>\n",
       "      <td>146.99</td>\n",
       "      <td>8.29</td>\n",
       "      <td>231.97</td>\n",
       "      <td>91.63</td>\n",
       "      <td>120.32</td>\n",
       "      <td>102.50</td>\n",
       "      <td>13.65</td>\n",
       "      <td>156.37</td>\n",
       "      <td>82.03</td>\n",
       "      <td>...</td>\n",
       "      <td>63.27</td>\n",
       "      <td>43.51</td>\n",
       "      <td>45.97</td>\n",
       "      <td>35.57</td>\n",
       "      <td>70.47</td>\n",
       "      <td>90.41</td>\n",
       "      <td>33.65</td>\n",
       "      <td>29.67</td>\n",
       "      <td>199.80</td>\n",
       "      <td>41691.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>39.16</td>\n",
       "      <td>141.32</td>\n",
       "      <td>7.64</td>\n",
       "      <td>230.71</td>\n",
       "      <td>89.54</td>\n",
       "      <td>120.20</td>\n",
       "      <td>101.16</td>\n",
       "      <td>13.25</td>\n",
       "      <td>155.14</td>\n",
       "      <td>80.94</td>\n",
       "      <td>...</td>\n",
       "      <td>62.78</td>\n",
       "      <td>42.76</td>\n",
       "      <td>46.18</td>\n",
       "      <td>35.01</td>\n",
       "      <td>68.15</td>\n",
       "      <td>90.23</td>\n",
       "      <td>31.46</td>\n",
       "      <td>28.50</td>\n",
       "      <td>196.75</td>\n",
       "      <td>40661.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            KIMBERA MF Equity  GAPB MF Equity  CEMEXCPO MF Equity  \\\n",
       "DATE                                                                \n",
       "2016-01-01              40.37          152.05                8.72   \n",
       "2016-01-04              39.16          148.57                8.43   \n",
       "2016-01-05              38.89          147.93                8.31   \n",
       "2016-01-06              39.13          146.99                8.29   \n",
       "2016-01-07              39.16          141.32                7.64   \n",
       "\n",
       "            ASURB MF Equity  GFNORTEO MF Equity  KOFUBL MF Equity  \\\n",
       "DATE                                                                \n",
       "2016-01-01           243.89               94.99            123.90   \n",
       "2016-01-04           238.29               91.79            120.29   \n",
       "2016-01-05           233.03               91.72            118.82   \n",
       "2016-01-06           231.97               91.63            120.32   \n",
       "2016-01-07           230.71               89.54            120.20   \n",
       "\n",
       "            AC* MF Equity  LABB MF Equity  FEMSAUBD MF Equity  OMAB MF Equity  \\\n",
       "DATE                                                                            \n",
       "2016-01-01         104.68           13.85              161.63           83.61   \n",
       "2016-01-04         102.09           14.41              157.62           81.77   \n",
       "2016-01-05         101.12           13.77              157.80           81.58   \n",
       "2016-01-06         102.50           13.65              156.37           82.03   \n",
       "2016-01-07         101.16           13.25              155.14           80.94   \n",
       "\n",
       "            ...  MEGACPO MF Equity  GCC* MF Equity  BIMBOA MF Equity  \\\n",
       "DATE        ...                                                        \n",
       "2016-01-01  ...              64.19           44.81             45.95   \n",
       "2016-01-04  ...              64.45           43.51             45.02   \n",
       "2016-01-05  ...              63.16           43.51             45.77   \n",
       "2016-01-06  ...              63.27           43.51             45.97   \n",
       "2016-01-07  ...              62.78           42.76             46.18   \n",
       "\n",
       "            GMEXICOB MF Equity  GCARSOA1 MF Equity  TLEVICPO MF Equity  \\\n",
       "DATE                                                                     \n",
       "2016-01-01               36.79               70.98               94.34   \n",
       "2016-01-04               35.71               69.83               92.62   \n",
       "2016-01-05               36.45               70.70               91.89   \n",
       "2016-01-06               35.57               70.47               90.41   \n",
       "2016-01-07               35.01               68.15               90.23   \n",
       "\n",
       "            ALFAA MF Equity  BSMXB MF Equity  PINFRA* MF Equity  MEXBOL INDEX  \n",
       "DATE                                                                           \n",
       "2016-01-01            34.10            30.25             202.73      42977.50  \n",
       "2016-01-04            33.74            29.46             202.93      42113.70  \n",
       "2016-01-05            33.99            29.72             203.56      42041.68  \n",
       "2016-01-06            33.65            29.67             199.80      41691.19  \n",
       "2016-01-07            31.46            28.50             196.75      40661.57  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipc_path = \"ipc.xlsx\" \n",
    "market = pd.read_excel(ipc_path, index_col=0)\n",
    "market.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmkt = market.pct_change().dropna()\n",
    "\n",
    "Xall = Rmkt.drop(\"MEXBOL INDEX\", axis=1)\n",
    "Xall = Xall.join(Rmkt[\"MEXBOL INDEX\"].shift(1)).dropna()\n",
    "\n",
    "Xtrain = Xall.drop(\"MEXBOL INDEX\", axis=1)\n",
    "ytrain = Xall[\"MEXBOL INDEX\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_period = \"2019-12\"\n",
    "Xtest, ytest = Xtrain[test_period:], ytrain[test_period:]\n",
    "Xtrain, ytrain = Xtrain[:\"2019-11\":], ytrain[:\"2019-11\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import uniform\n",
    "alpha_list = uniform(0.0001, 0.000001, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.00022218025721252883, tolerance: 4.3141354023829896e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.00014125278683041042, tolerance: 3.979933643198603e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.621598780520496e-05, tolerance: 3.89133584643056e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.00014876642633271722, tolerance: 3.79106713286149e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.570865420838732e-05, tolerance: 4.3141354023829896e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.376227856680166e-05, tolerance: 3.89133584643056e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.185544779518712e-05, tolerance: 3.79106713286149e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.237888111460896e-05, tolerance: 4.3141354023829896e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0047938052798692e-05, tolerance: 3.979933643198603e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.511887586854761e-05, tolerance: 3.89133584643056e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.427653467191158e-05, tolerance: 3.79106713286149e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.511563895073238e-05, tolerance: 4.3141354023829896e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4553962199625606e-05, tolerance: 3.979933643198603e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.9706971457020325e-05, tolerance: 3.89133584643056e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.623314777607064e-05, tolerance: 3.79106713286149e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.022784431404863e-05, tolerance: 4.3141354023829896e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.432610958631568e-06, tolerance: 3.89133584643056e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.326207647117723e-06, tolerance: 3.79106713286149e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0001862175422519749, tolerance: 4.3141354023829896e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0001687875042778019, tolerance: 3.979933643198603e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.466501010430134e-05, tolerance: 3.89133584643056e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.00015309362426377433, tolerance: 3.79106713286149e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.900026032278072e-05, tolerance: 4.3141354023829896e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6780037266015888e-05, tolerance: 3.979933643198603e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4194975365099403e-05, tolerance: 3.89133584643056e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.892649241828659e-05, tolerance: 3.79106713286149e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0468563411187085e-05, tolerance: 4.3141354023829896e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.498009269240035e-06, tolerance: 3.79106713286149e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.297818639518094e-05, tolerance: 4.3141354023829896e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.51225794687338e-06, tolerance: 3.979933643198603e-06\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9800881124800232e-05, tolerance: 3.89133584643056e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.454318200051294e-05, tolerance: 3.79106713286149e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.260519971827214e-05, tolerance: 4.3141354023829896e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.65088308283049e-06, tolerance: 3.979933643198603e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.987508905631173e-05, tolerance: 3.89133584643056e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.322754009430593e-05, tolerance: 3.79106713286149e-06\n",
      "  positive)\n",
      "C:\\Users\\Palomino\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Usamos modelo Lasso y el polinomio segundo grado\n",
    "pipe = Pipeline([\n",
    "    (\"paso1\", PolynomialFeatures(degree=2)),\n",
    "    (\"model\", Lasso())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    \"model__normalize\": [True, False],\n",
    "    \"model__alpha\": alpha_list\n",
    "}\n",
    "\n",
    "# Entrenamos\n",
    "gcv = GridSearchCV(estimator=pipe, param_grid= dict(params), cv=5)\n",
    "gs=gcv.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__alpha': 8.835301946382197e-06, 'model__normalize': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        , -0.03982707,  0.03015905, -0.        , -0.        ,\n",
       "        0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "        0.0001816 , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.0040021 , -0.        ,\n",
       "        0.        ,  0.03708058,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.01884646,  0.        ,  0.        , -0.00965994,\n",
       "        0.03523089, -0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.        , -0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
       "        0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.        , -0.        ,  0.        ,  0.        , -0.        ,\n",
       "       -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "        0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "       -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "       -0.        , -0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        ,  0.        ,  0.        , -0.        ,\n",
       "       -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "        0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "        0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "       -0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "        0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "       -0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "        0.        , -0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "       -0.        , -0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.        ,  0.        ,  0.        , -0.        ,\n",
       "       -0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "       -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        ,  0.        , -0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        , -0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        , -0.        ,  0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "       -0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "        0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "       -0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "       -0.        , -0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "       -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_\n",
    "gs.best_estimator_[1].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.Series(gs.best_estimator_[1].coef_[0:35], Xtrain.columns)\n",
    "res = res.reset_index().rename({\"index\": \"asset\", 0: \"value\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(res, x=\"asset\", y=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_sig=list(filter(lambda x: x != 0, gs.best_estimator_[1].coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(coef_sig, x=range(len(coef_sig)), y=coef_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los coeficientes significativos mayores a 1e-2 son:  4 \n",
      "Los cuales se muestran en la sigueintes lista: \n",
      " [0.030126819891901534, 0.03705666102434287, 0.018809934619132992, 0.035195448405082076]\n"
     ]
    }
   ],
   "source": [
    "coef1=list(filter(lambda x: x > 1e-2, gs.best_estimator_[1].coef_))\n",
    "print(\"Los coeficientes significativos mayores a 1e-2 son: \", len(coef1), \"\\nLos cuales se muestran en la sigueintes lista: \\n\" ,coef1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
